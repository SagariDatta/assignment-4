{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Exploring Yelp Reviews\n",
    "\n",
    "**NOTE: THIS ASSIGNMENT IS OPTIONAL**\n",
    "\n",
    "Your final grade will be based on your highest assignment grades, so this assignment can be used to replace any of your other (lower-graded) assignments. \n",
    "\n",
    "**Due date: Friday, 3/15 by 5pm**\n",
    "\n",
    "In this assignment, we'll explore restaurant review data available through the [Yelp Dataset Challenge](https://www.yelp.com/dataset/challenge). The dataset includes Yelp data for user reviews and business information for 10 metropolitan areas. The `data` directory in this repository includes data files for reviews and restaurants in 3 of these cities: Cleveland, Pittsburgh, and Charlotte. These cities were chosen since the data is not too large â€” the data for the other cities can be downloaded from the Yelp download page. For this assignment, you are welcome to analyze data any of the three cities. \n",
    "\n",
    "This assignment is broken into two parts:\n",
    "\n",
    "#### Part 1: testing how well sentiment analysis works.\n",
    "\n",
    "Because Yelp reviews include the number of stars given by the user, the Yelp data set provides a unique opportunity to test how well our sentiment analysis works by comparing the number of stars to the polarity of reviews.\n",
    "\n",
    "#### Part 2: analyzing correlations between restaurant reviews and census data\n",
    "\n",
    "Analyzing the frequencies of different sets of words in Yelp reviews can offer insight into urban culture. Specifically, we'll test the correlation between household income (using census data) and different types of restaurants, extracted from review data.\n",
    "    \n",
    "#### Background readings\n",
    "- [Does sentiment analysis work?](http://varianceexplained.org/r/yelp-sentiment/)\n",
    "- [The Geography of Taste: Using Yelp to Study Urban Culture](https://www.mdpi.com/2220-9964/7/9/376/pdf/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Does Sentiment Analysis Work?\n",
    "\n",
    "In this part, we'll load the data, perform a sentiment analysis, and explore the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load review data\n",
    "\n",
    "You can choose data from Cleveland, Charlotte, or Pittsburgh. The data is stored as a JSON file and you can use `pandas.read_json` to load it. \n",
    "\n",
    "**Notes**\n",
    "\n",
    "The JSON data is in a \"records\" format. To load it, you'll need to pass the following keywords: \n",
    "\n",
    "- `orient='records'`\n",
    "- `lines=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Format the review text\n",
    "\n",
    "The first step is to split the review text into its individual words and make all of the words lower-cased.\n",
    "\n",
    "Add a new column, called 'formatted_text', which each entry is a list of the lower-cased words in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Review stop words\n",
    "\n",
    "Use the `nltk` library to remove any stop words from the list of words in each review. \n",
    "\n",
    "Overwrite the 'formatted_text' column to contain a list of lower-cased words in each review, with no stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Calculate polarity and subjectivity\n",
    "\n",
    "Using the formatted text column, create a list of `textblob.TextBlob()` objects and then extract the `subjectivity` and `polarity`.\n",
    "\n",
    "Add two new columns to the review DataFrame: `polarity` and `subjectivity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Comparing the sentiment analysis to number of stars \n",
    "\n",
    "Use `seaborn` to make two box plots, one showing the polarity vs number of user stars and one showing the subjectivity vs the number of user stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do your charts indicate for the effectiveness of our sentiment analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 The importance of individual words\n",
    "\n",
    "In this part, we'll explore the importance and frequency of individual words in Yelp reviews.\n",
    "\n",
    "We will identify the most common reviews and then plot the average polarity vs the user stars for the reviews where those words occur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.1 Select a random sample of the review data\n",
    "\n",
    "Select 1,000 random rows from the DataFrame holding the review data. Use the `.sample()` function to perform the selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2 Re-format the data\n",
    "\n",
    "Pass the subset of review data from the previous part to the `reshape_data()` function defined below. Explore the result of this function, and in one or two sentences, explain the operation performed by `reshape_data()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(review_subset):\n",
    "    \"\"\"\n",
    "    Reshape the input dataframe of review data.\n",
    "    \"\"\"\n",
    "    from pandas import Series, merge\n",
    "    \n",
    "    X = (review_subset['formatted_text']\n",
    "         .apply(Series)\n",
    "         .stack()\n",
    "         .reset_index(level=1, drop=True)\n",
    "         .to_frame('word'))\n",
    "    \n",
    "    \n",
    "    R = review_subset[['polarity', 'stars', 'review_id']]\n",
    "    \n",
    "    return merge(R, X, left_index=True, right_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** what is the operation performed by the `reshape_data()` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.3 Calculate the average number of stars and polarity for each word\n",
    "\n",
    "Using the result from 1.6.2, group by the \"word\" column, and calculate the following three quantities:\n",
    "1. the size of each group \n",
    "1. the average number of user stars for each word\n",
    "1. the average polarity for each word\n",
    "\n",
    "Combine these three results into a single DataFrame object.  \n",
    "\n",
    "**Hint:** you can combine the three results using either the `pandas.concat()` or the `pandas.merge()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.4 Select words the occur at least 50 times in reviews\n",
    "\n",
    "Trim your DataFrame from the last section to only include words that occurred at least 50 times. Remember, when you grouped by the 'word' column, the `size()` function told you how many times each word occurred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.5 Plot the average polarity vs user stars\n",
    "\n",
    "Use `matplotlib` to make a scatter plot of the average user stars vs average polarity for the words in the data frame from the last section. This will involve two steps:\n",
    "\n",
    "Loop over each row of the data frame from the last section and for each row:\n",
    "\n",
    "1. Use `plt.scatter(x, y)` to plot a scatter plot, where x is polarity and y is stars. \n",
    "1. Use `plt.text(x, y, word)` to add the corresponding word to each scatter marker.\n",
    "\n",
    "Using the data frame from section 1.4, add vertical and horizontal lines to your chart that shows the average number of user stars and the average polarity across all reviews in the data set. \n",
    "\n",
    "Make sure the figure is big enough so that you can make out some of the words, especially at low and high polarity values. You should be able to see a strong trend between polarity and user stars, and some of the most common words occurring in these reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlating restaurant data and household income\n",
    "\n",
    "In this part, we'll use the census API to download household income data and overlay restaurant locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Query the Census API\n",
    "\n",
    "Use the `census_area` package to download total household income by census tract (with tract geometries) from the 2017 ACS data set. You can identify the proper variable name from:\n",
    "\n",
    "https://api.census.gov/data/2017/acs/acs5/variables.html\n",
    "\n",
    "**Hints**\n",
    "\n",
    "The FIPS codes for the various cities are: \n",
    "\n",
    "- Pittsburgh\n",
    "    - PA code: '42'\n",
    "    - City code: '61000'\n",
    "- Cleveland\n",
    "    - OH code: '39'\n",
    "    - City code: '16000'\n",
    "- Charlotte\n",
    "    - NC code: '37'\n",
    "    - City code: '12000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Convert the GeoJSON features to a GeoDataFrame\n",
    "\n",
    "Be sure to use the proper CRS for census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Plot a choropleth map of the household income\n",
    "\n",
    "Use the built-in `geopandas` `plot()` function. \n",
    "\n",
    "Be sure to convert to a reasonable CRS first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Load the restaurants data\n",
    "\n",
    "Use the `latitude` and `longitude` columns to create a GeoDataFrame after loading the JSON data.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "The JSON data is in a \"records\" format. To load it, you'll need to pass the following keywords: \n",
    "\n",
    "- `orient='records'`\n",
    "- `lines=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Overlay restaurants on the income map\n",
    "\n",
    "Overlay the restaurants and color the points according to the 'stars' column. \n",
    "\n",
    "You can use the 'coolwarm' color map: blue points will have below-average reviews and red points will have above-average stars. \n",
    "\n",
    "**Hint**\n",
    "\n",
    "You can use the `.geometry.total_bounds` attribute to get the axes limits of the city's census tracts. \n",
    "\n",
    "```python\n",
    "[xmin, ymin, xmax, ymax] = income.geometry.total_bounds\n",
    "```\n",
    "\n",
    "You can then use these limits to set the matplotlib plot limits accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Comparing polarity vs. stars geographically\n",
    "\n",
    "- Merge the restaurants GeoDataFrame with the DataFrame with the 'polarity' column for each review. \n",
    "- Make a side-by-side plot with two columns: one subplot shows hex bins giving the polarity of the restaurant review and the other shows hex bins giving the number of stars\n",
    "\n",
    "As we saw in Section 1, you should see strong correlation between the two subplots. \n",
    "\n",
    "**Hints**\n",
    "- The 'business_id' column should be present in both the data frames holding review data and restaurant data.\n",
    "- See the `plt.subplots()` function for creating a figure with 2 subplots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
